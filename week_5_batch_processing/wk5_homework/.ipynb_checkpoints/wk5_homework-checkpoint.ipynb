{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bec352bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b2264a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/27 22:08:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('Week 5 Homework') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e857f9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question 1\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b15d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 2\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('/Users/alvin/Documents/de-zoomcamp/week_5_batch_processing/csv/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5765758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1f6d927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc7d3f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,StringType,true),StructField(DOLocationID,StringType,true),StructField(SR_Flag,StringType,true)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb1fd514",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51da093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('/Users/alvin/Documents/de-zoomcamp/week_5_batch_processing/csv/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fcd89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cbf06de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/27 22:21:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/02/27 22:21:55 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "22/02/27 22:21:56 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.repartition(24).write.parquet('/Users/alvin/Documents/de-zoomcamp/week_5_batch_processing/wk5_homework/hvhs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8da24507",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output 206.7 Mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1da92ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 3\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('/Users/alvin/Documents/de-zoomcamp/week_5_batch_processing/csv/fhvhv_tripdata_2021-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25500e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:26:02|2021-02-01 00:42:51|         208|         243|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:45:50|2021-02-01 01:02:50|         243|         220|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:06:42|2021-02-01 00:31:50|          49|          37|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:34:34|2021-02-01 00:58:13|          37|          76|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:03:43|2021-02-01 00:39:37|          80|         241|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:55:36|2021-02-01 01:08:39|         174|          51|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:06:13|2021-02-01 00:33:45|         235|         129|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:42:24|2021-02-01 01:11:31|         129|         169|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:07:05|2021-02-01 00:20:53|         226|          82|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:28:56|2021-02-01 00:33:59|          82|         129|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:44:53|2021-02-01 01:07:54|           7|          79|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:17:55|2021-02-01 00:34:41|           4|         170|   null|\n",
      "|           HV0003|              B02888|2021-02-01 00:38:14|2021-02-01 00:59:20|         164|          42|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:08:04|2021-02-01 00:24:41|         237|           4|   null|\n",
      "|           HV0004|              B02800|2021-02-01 00:30:44|2021-02-01 00:41:26|         107|          45|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "974e623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8f6b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 18:==================================================>       (7 + 1) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2021-02-15'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04f39e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|pickup_datetime|           time_diff|\n",
      "+---------------+--------------------+\n",
      "|     2021-02-11|INTERVAL '0 20:59...|\n",
      "|     2021-02-17|INTERVAL '0 15:53...|\n",
      "|     2021-02-20|INTERVAL '0 12:13...|\n",
      "|     2021-02-03|INTERVAL '0 11:17...|\n",
      "|     2021-02-19|INTERVAL '0 10:26...|\n",
      "|     2021-02-25|INTERVAL '0 09:43...|\n",
      "|     2021-02-20|INTERVAL '0 09:40...|\n",
      "|     2021-02-18|INTERVAL '0 09:36...|\n",
      "|     2021-02-18|INTERVAL '0 09:35...|\n",
      "|     2021-02-10|INTERVAL '0 09:29...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:===========================================>              (6 + 2) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Question 4\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE(pickup_datetime),\n",
    "    (dropoff_datetime-pickup_datetime) AS time_diff\n",
    "FROM\n",
    "    trips\n",
    "ORDER BY\n",
    "    time_diff DESC\n",
    "LIMIT 10;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13132944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|dispatching_base_num|count(1)|\n",
      "+--------------------+--------+\n",
      "|              B02510| 3233664|\n",
      "|              B02764|  965568|\n",
      "|              B02872|  882689|\n",
      "|              B02875|  685390|\n",
      "|              B02765|  559768|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:=====================>                                    (3 + 5) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Question 5\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num,\n",
    "    COUNT(*)\n",
    "FROM\n",
    "    trips\n",
    "GROUP BY\n",
    "    dispatching_base_num\n",
    "ORDER BY\n",
    "    COUNT(*) DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb832f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 6\n",
    "zones = spark.read \\\n",
    "    .option(\"header\",'true') \\\n",
    "    .csv('/Users/alvin/Documents/de-zoomcamp/week_5_batch_processing/csv/taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "788b9c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "febb0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1764227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 84:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "| pickup_dropoff_pair|count(1)|\n",
      "+--------------------+--------+\n",
      "|East New York / E...|   45041|\n",
      "|Borough Park / Bo...|   37329|\n",
      "| Canarsie / Canarsie|   28026|\n",
      "|Crown Heights Nor...|   25976|\n",
      "|Bay Ridge / Bay R...|   17934|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 84:==============>                                           (2 + 6) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CONCAT(pu_zone.Zone, ' / ', do_zone.Zone) AS pickup_dropoff_pair,\n",
    "    COUNT(1)\n",
    "FROM\n",
    "    trips\n",
    "INNER JOIN\n",
    "    zones AS pu_zone\n",
    "ON \n",
    "    trips.PULocationID = pu_zone.LocationID\n",
    "INNER JOIN\n",
    "    zones AS do_zone\n",
    "ON\n",
    "    trips.DOLocationID = do_zone.LocationID\n",
    "GROUP BY\n",
    "    pickup_dropoff_pair\n",
    "ORDER BY \n",
    "    COUNT(1) DESC\n",
    "LIMIT 5;\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
